{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SageMaker Bring Your Own Model - NVIDIA Model\n",
    "__Blah blah blah__  \n",
    "[End-to-End Deep Learning for Self-Driving Cars](https://devblogs.nvidia.com/deep-learning-self-driving-cars/)\n",
    "![Architecture](https://devblogs.nvidia.com/parallelforall/wp-content/uploads/2016/08/cnn-architecture-624x890.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Functions\n",
    "__blah blah blah__  \n",
    "\n",
    "<details><summary><b>Note to self</b></summary><p>\n",
    "Make sure to highlight the fact that the necessary libraries have already been configured in the model temaplate file.\n",
    "</p></details>\n",
    "\n",
    "\n",
    "### Building the model: `build_model()` Function\n",
    "__Blah blah blah__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    \"\"\"\n",
    "    Create the NVidia Model.\n",
    "    \"\"\"\n",
    "    net = None\n",
    "\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary><b>Solution (Click to expand)</b></summary>\n",
    "<p>\n",
    "```\n",
    "def build_model():\n",
    "    \"\"\"\n",
    "    Create the NVidia Model.\n",
    "    \"\"\"\n",
    "    net = gluon.nn.Sequential()\n",
    "    with net.name_scope():\n",
    "        net.add(gluon.nn.Lambda(lambda x: x/127.5-1.0)) #Normalization\n",
    "        net.add(gluon.nn.Conv2D(channels=24, kernel_size=(5, 5), strides=(2, 2), padding=1, activation='relu'))\n",
    "        net.add(gluon.nn.Conv2D(channels=36, kernel_size=(5, 5), strides=(2, 2), padding=1, activation='relu'))\n",
    "        net.add(gluon.nn.Conv2D(channels=48, kernel_size=(5, 5), strides=(2, 2), padding=1, activation='relu'))\n",
    "        net.add(gluon.nn.Conv2D(channels=64, kernel_size=3, activation='relu'))\n",
    "        net.add(gluon.nn.Conv2D(channels=64, kernel_size=3, activation='relu'))\n",
    "        net.add(gluon.nn.Flatten())\n",
    "        net.add(gluon.nn.Dense(1164))\n",
    "        net.add(gluon.nn.Activation('relu'))\n",
    "        net.add(gluon.nn.Dense(100))\n",
    "        net.add(gluon.nn.Activation('relu'))\n",
    "        net.add(gluon.nn.Dense(50))\n",
    "        net.add(gluon.nn.Activation('relu'))\n",
    "        net.add(gluon.nn.Dense(10))\n",
    "        net.add(gluon.nn.Activation('relu'))\n",
    "        net.add(gluon.nn.Dense(1))\n",
    "    net.hybridize()\n",
    "    return net\n",
    "```\n",
    "</p>\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resizing the Input Shapes: `transform()` Function\n",
    "__BLAH BLAH BLAH__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(x, y):\n",
    "    \"\"\"\n",
    "    Reshape the numpy arrays as 4D Tensors for MXNet.\n",
    "    \n",
    "    Arguments:\n",
    "    x -- Numpy Array of input images\n",
    "    y -- Numpy Array of labels\n",
    "    \n",
    "    Returns:\n",
    "    x -- Numpy Array as (NCHW).\n",
    "    y -- Label as Column vector.\n",
    "    \"\"\"\n",
    "    data  = None\n",
    "    label = None\n",
    "    return data, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary><b>Solution (Click to expand)</b></summary><p>\n",
    "```\n",
    "def transform(x, y):\n",
    "    \"\"\"\n",
    "    Reshape the numpy arrays as 4D Tensors for MXNet.\n",
    "    \n",
    "    Arguments:\n",
    "    x -- Numpy Array of input images\n",
    "    y -- Numpy Array of labels\n",
    "    \n",
    "    Returns:\n",
    "    x -- Numpy Array as (NCHW).\n",
    "    y -- Label as Column vector.\n",
    "    \"\"\"\n",
    "    data  = x.reshape(-1, 3, 66, 200)\n",
    "    label = y.reshape(-1, 1)\n",
    "    return data, label\n",
    "```\n",
    "<\\p></details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and Transforming the Datasets: `load_data()` Function\n",
    "__blah blah blah__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(f_path):\n",
    "    \"\"\"\n",
    "    Retrieves and loads the training/testing data from S3.\n",
    "    \n",
    "    Arguments:\n",
    "    f_path -- Location for the training/testing input dataset.\n",
    "    \n",
    "    Returns:\n",
    "    Resized training and testing data along with training and testing labels.\n",
    "    \"\"\"\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary><b>Solution (Click to expand)</b></summary><p>\n",
    "```\n",
    "def load_data(f_path):\n",
    "    \"\"\"\n",
    "    Retrieves and loads the training/testing data from S3.\n",
    "    \n",
    "    Arguments:\n",
    "    f_path -- Location for the training/testing input dataset.\n",
    "    \n",
    "    Returns:\n",
    "    Pre-processed training and testing data along with training and testing labels.\n",
    "    \"\"\"\n",
    "    train_x = np.load(f_path+'/train_X.npy')\n",
    "    train_y = np.load(f_path+'/train_Y.npy')\n",
    "    train_X, train_Y = transform(train_x, train_y)\n",
    "    test_x = np.load(f_path+'/valid_X.npy')\n",
    "    test_y = np.load(f_path+'/valid_Y.npy')\n",
    "    test_X, test_Y = transform(test_x, test_y)\n",
    "    return train_X, train_Y, test_X, test_Y\n",
    "```\n",
    "</p></details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measuring the Training vs. Validation Accuracy: `accuracy()` Function\n",
    "__blah blah blah__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(data_iterator, net, ctx):\n",
    "    \"\"\"\n",
    "    Evaluates the Accuracy of the model against the Training or Testing iterator.\n",
    "    \n",
    "    Arguments:\n",
    "    data_iterator -- Iterator.\n",
    "    net -- Gluon Model.\n",
    "    \n",
    "    Returns:\n",
    "    Accuracy of the model against the data iterator.\n",
    "    \"\"\"\n",
    "    acc = mx.metric.Accuracy()\n",
    "    for i, (data, label) in enumerate(data_iterator):\n",
    "        data = data.as_in_context(ctx)\n",
    "        label = label.as_in_context(ctx)\n",
    "        output = net(data)\n",
    "        predictions = nd.argmax(output, axis=1)\n",
    "        acc.update(preds=predictions, labels=label)\n",
    "    return acc.get()[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the Model: `train()` Function\n",
    "__blah blah blah__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(channel_input_dirs, hyperparameters, hosts, num_gpus, output_data_dir, **kwargs):\n",
    "    \"\"\"\n",
    "    BLAH BLAH BLAH\n",
    "\n",
    "    \"\"\"\n",
    "    # Set the Context\n",
    "    \n",
    "    # Set Local vs. Distributed training\n",
    "    if len(hosts) == 1:\n",
    "        kvstore = 'device' if num_gpus > 0 else 'local'\n",
    "    else:\n",
    "        kvstore = 'dist_device_sync' if num_gpus > 0 else 'dist_sync'\n",
    "    \n",
    "    # Load hyperparameters\n",
    "    \n",
    "    # Load Training/Testing Data\n",
    "    \n",
    "    # Create Training and Test Data Iterators\n",
    "    train_data = None\n",
    "    test_data = None\n",
    "    \n",
    "    # Initialize the neural network structure\n",
    "    net = build_model()\n",
    "    \n",
    "    # Parameter Initialization\n",
    "    \n",
    "    # Optimizer\n",
    "    \n",
    "    # Squared Error Loss Function\n",
    "    square_loss = None\n",
    "    \n",
    "    # Train the model\n",
    "    for epoch in range(epochs):\n",
    "        curr_loss = 0\n",
    "        for i, (data, label) in enumerate(train_data):\n",
    "            data = None\n",
    "            label = None\n",
    "            with autograd.record():\n",
    "                output = net(data)\n",
    "                loss = square_loss(output, label)\n",
    "            loss.backward()\n",
    "            trainer.step(data.shape[0])\n",
    "            curr_loss += nd.mean(loss).asscalar()\n",
    "        val_accuracy = accuracy(test_data, net, ctx)\n",
    "        train_accuracy = accuracy(train_data, net, ctx)\n",
    "        print(\n",
    "            \"Epoch {}: Loss: {}; Training Accuracy = {}; Validation Accuracy = {}\"\\\n",
    "            .format(epoch, curr_loss/len(train_data), train_accuracy, val_accuracy)\n",
    "        )\n",
    "\n",
    "    # Return the model for saving\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary><b>Solution (Click to expand)</b></summary><p>\n",
    "```\n",
    "def train(channel_input_dirs, hyperparameters, hosts, num_gpus, output_data_dir, **kwargs):\n",
    "    \"\"\"\n",
    "    blah blah blah\n",
    "    \"\"\"\n",
    "    # Set the Context\n",
    "    ctx = mx.gpu() if num_gpus > 0 else mx.cpu()\n",
    "    \n",
    "    # Set Local vs. Distributed training\n",
    "    if len(hosts) == 1:\n",
    "        kvstore = 'device' if num_gpus > 0 else 'local'\n",
    "    else:\n",
    "        kvstore = 'dist_device_sync' if num_gpus > 0 else 'dist_sync'\n",
    "    \n",
    "    # Load hyperparameters\n",
    "    epochs = hyperparameters.get('epochs', 11)\n",
    "    optmizer = hyperparameters.get('optmizer', 'adam')\n",
    "    lr = hyperparameters.get('learning_rate', 1.0e-4)\n",
    "    batch_size = hyperparameters.get('batch_size', 64)\n",
    "    \n",
    "    # Load Training/Testing Data\n",
    "    f_path = channel_input_dirs['training']\n",
    "    train_X, train_Y, test_X, test_Y = load_data(f_path)\n",
    "    \n",
    "    # Create Training and Test Data Iterators\n",
    "    train_data = mx.gluon.data.DataLoader(\n",
    "        mx.gluon.data.ArrayDataset(\n",
    "            train_X,\n",
    "            train_Y\n",
    "        ),\n",
    "        shuffle=True,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    test_data = mx.gluon.data.DataLoader(\n",
    "        mx.gluon.data.ArrayDataset(\n",
    "            test_X,\n",
    "            test_Y\n",
    "        ),\n",
    "        shuffle=False,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    \n",
    "    # Initialize the neural network structure\n",
    "    net = build_model()\n",
    "    \n",
    "    # Parameter Initialization\n",
    "    net.collect_params().initialize(mx.init.Xavier(magnitude=2.24), ctx=ctx)\n",
    "    \n",
    "    # Optimizer\n",
    "    trainer = gluon.Trainer(net.collect_params(), optmizer, {'learning_rate': lr})\n",
    "    \n",
    "    # Squared Error Loss Function\n",
    "    square_loss = gluon.loss.L2Loss()\n",
    "    \n",
    "    # Train the model\n",
    "    for epoch in range(epochs):\n",
    "        curr_loss = 0\n",
    "        for i, (data, label) in enumerate(train_data):\n",
    "            data = data.as_in_context(ctx)\n",
    "            label = label.as_in_context(ctx)\n",
    "            with autograd.record():\n",
    "                output = net(data)\n",
    "                loss = square_loss(output, label)\n",
    "            loss.backward()\n",
    "            trainer.step(data.shape[0])\n",
    "            curr_loss += nd.mean(loss).asscalar()\n",
    "        val_accuracy = accuracy(test_data, net, ctx)\n",
    "        train_accuracy = accuracy(train_data, net, ctx)\n",
    "        print(\"Epoch {}: Loss: {}; Training Accuracy = {}; Validation Accuracy = {}\"\\\n",
    "              .format(epoch, curr_loss/len(train_data), train_accuracy, val_accuracy)\n",
    "             )\n",
    "\n",
    "    # Return the model for saving\n",
    "    return net\n",
    "```\n",
    "</p></details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the Model for Inference: `save()` Function\n",
    "__blah blah blah__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(net, model_dir):\n",
    "    \"\"\"\n",
    "    Saves the trained model to S3.\n",
    "    \n",
    "    Arguments:\n",
    "    model -- The model returned from the `train()` function.\n",
    "    model_dir -- The model directory location to save the model.\n",
    "    \"\"\"\n",
    "    print(\"Saving the trained model ...\")\n",
    "    y = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary><b>Solution (Click to expand)</b></summary><p>\n",
    "```\n",
    "def save(net, model_dir):\n",
    "    \"\"\"\n",
    "    Saves the trained model to S3.\n",
    "    \n",
    "    Arguments:\n",
    "    model -- The model returned from the `train()` function.\n",
    "    model_dir -- The model directory location to save the model.\n",
    "    \"\"\"\n",
    "    print(\"Saving the trained model ...\")\n",
    "    y = net(mx.sym.var('data'))\n",
    "    y.save('%s/model.json' % model_dir)\n",
    "    net.collect_params().save('%s/model.params' % model_dir)\n",
    "```\n",
    "</p></details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Inference Functions\n",
    "__blah blah blah__\n",
    "\n",
    "### Model Hosting: `model_fn` Function\n",
    "__blah blah blah; even though it won't be used just yet__  \n",
    "\n",
    "<details><summary><b>Note to self</b></summary><p>\n",
    "Test this funciton code below before completion!\n",
    "</p></details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn(model_dir):\n",
    "    \"\"\"\n",
    "    Load the Gluon model for hosting.\n",
    "\n",
    "    Arguments:\n",
    "    model_dir -- SageMaker model directory.\n",
    "\n",
    "    Retuns:\n",
    "    Gluon model\n",
    "    \"\"\"\n",
    "    # Load the saved Gluon model\n",
    "    symbol = mx.sym.load('%s/model.json' % model_dir)\n",
    "    outputs = mx.sym.sigmoid(data=symbol, name='sigmoid_label')\n",
    "    inputs = mx.sym.var('data')\n",
    "    param_dict = gluon.ParameterDict('model_')\n",
    "    net = gluon.SymbolBlock(outputs, inputs, param_dict)\n",
    "    net.load_params('%s/model.params' % model_dir, ctx=mx.cpu())\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction Result: `transform_fn()` Function\n",
    "__blah blah blah__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_fn(net, data, input_content_type, output_content_type):\n",
    "    \"\"\"\n",
    "    Transform input data into prediction result.\n",
    "\n",
    "    Argument:\n",
    "    net -- Gluon model loaded from `model_fn()` function.\n",
    "    data -- Input data from the `InvokeEndpoint` request.\n",
    "    input_content_type -- Content type of the request (JSON).\n",
    "    output_content_type -- Desired content type (JSON) of the repsonse.\n",
    "    \n",
    "    Returns:\n",
    "    JSON payload of the prediction result and content type.\n",
    "    \"\"\"\n",
    "    # Parse the data\n",
    "    parsed = loads(data)\n",
    "    # Convert input to MXNet NDArray\n",
    "    nda = mx.nd.array(parsed)\n",
    "    output = net(nda)\n",
    "    prediction = nd.argmax(output, axis=1)\n",
    "    response_body = dumps(prediction.asnumpy().tolist()[0])\n",
    "    return response_body, output_content_type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Building and Training the SageMaker Estimator\n",
    "__Blah blah blah__\n",
    "### Building the Estimator Training Script\n",
    "__blah blah blah; copy your functions to `model.py`; should look as follows:__  \n",
    "\n",
    "<details><summary><b>`model.py` (Click to expand)</b></summary><p>\n",
    "```\n",
    "# Import necessary libraries\n",
    "import boto3\n",
    "import os\n",
    "import io\n",
    "import logging\n",
    "import datetime\n",
    "import json\n",
    "import mxnet as mx\n",
    "import numpy as np\n",
    "from json import dumps, loads\n",
    "from mxnet import nd, autograd, gluon\n",
    "\n",
    "# Set logging\n",
    "logging.getLogger(\"requests.packages.urllib3.connectionpool\").setLevel(logging.WARNING)\n",
    "\n",
    "# ---------------------------------------------------------------------------- #\n",
    "#                            Training functions                                #\n",
    "# ---------------------------------------------------------------------------- #\n",
    "def train(channel_input_dirs, hyperparameters, hosts, num_gpus, output_data_dir, **kwargs):\n",
    "    # Set the Context\n",
    "    ctx = mx.gpu() if num_gpus > 0 else mx.cpu()\n",
    "    \n",
    "    # Set Local vs. Distributed training\n",
    "    if len(hosts) == 1:\n",
    "        kvstore = 'device' if num_gpus > 0 else 'local'\n",
    "    else:\n",
    "        kvstore = 'dist_device_sync' if num_gpus > 0 else 'dist_sync'\n",
    "    \n",
    "    # Load hyperparameters\n",
    "    epochs = hyperparameters.get('epochs', 11)\n",
    "    optmizer = hyperparameters.get('optmizer', 'adam')\n",
    "    lr = hyperparameters.get('learning_rate', 1.0e-4)\n",
    "    batch_size = hyperparameters.get('batch_size', 64)\n",
    "    \n",
    "    # Load Training/Testing Data\n",
    "    f_path = channel_input_dirs['training']\n",
    "    train_X, train_Y, test_X, test_Y = load_data(f_path)\n",
    "    \n",
    "    # Create Training and Test Data Iterators\n",
    "    train_data = mx.gluon.data.DataLoader(\n",
    "        mx.gluon.data.ArrayDataset(\n",
    "            train_X,\n",
    "            train_Y\n",
    "        ),\n",
    "        shuffle=True,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    test_data = mx.gluon.data.DataLoader(\n",
    "        mx.gluon.data.ArrayDataset(\n",
    "            test_X,\n",
    "            test_Y\n",
    "        ),\n",
    "        shuffle=False,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    \n",
    "    # Initialize the neural network structure\n",
    "    net = build_model()\n",
    "    \n",
    "    # Parameter Initialization\n",
    "    net.collect_params().initialize(mx.init.Xavier(magnitude=2.24), ctx=ctx)\n",
    "    \n",
    "    # Optimizer\n",
    "    trainer = gluon.Trainer(net.collect_params(), optmizer, {'learning_rate': lr})\n",
    "    \n",
    "    # Squared Error Loss Function\n",
    "    square_loss = gluon.loss.L2Loss()\n",
    "    \n",
    "    # Train the model\n",
    "    for epoch in range(epochs):\n",
    "        curr_loss = 0\n",
    "        for i, (data, label) in enumerate(train_data):\n",
    "            data = data.as_in_context(ctx)\n",
    "            label = label.as_in_context(ctx)\n",
    "            with autograd.record():\n",
    "                output = net(data)\n",
    "                loss = square_loss(output, label)\n",
    "            loss.backward()\n",
    "            trainer.step(data.shape[0])\n",
    "            curr_loss += nd.mean(loss).asscalar()\n",
    "        val_accuracy = accuracy(test_data, net, ctx)\n",
    "        train_accuracy = accuracy(train_data, net, ctx)\n",
    "        print(\"Epoch {}: Loss: {}; Training Accuracy = {}; Validation Accuracy = {}\"\\\n",
    "              .format(epoch, curr_loss/len(train_data), train_accuracy, val_accuracy)\n",
    "             )\n",
    "\n",
    "    # Return the model for saving\n",
    "    return net\n",
    "\n",
    "def build_model():\n",
    "    \"\"\"\n",
    "    Create the NVidia Model.\n",
    "    \"\"\"\n",
    "    net = gluon.nn.Sequential()\n",
    "    with net.name_scope():\n",
    "        net.add(gluon.nn.Lambda(lambda x: x/127.5-1.0)) #Normalization\n",
    "        net.add(gluon.nn.Conv2D(channels=24, kernel_size=(5, 5), strides=(2, 2), padding=1, activation='relu'))\n",
    "        net.add(gluon.nn.Conv2D(channels=36, kernel_size=(5, 5), strides=(2, 2), padding=1, activation='relu'))\n",
    "        net.add(gluon.nn.Conv2D(channels=48, kernel_size=(5, 5), strides=(2, 2), padding=1, activation='relu'))\n",
    "        net.add(gluon.nn.Conv2D(channels=64, kernel_size=3, activation='relu'))\n",
    "        net.add(gluon.nn.Conv2D(channels=64, kernel_size=3, activation='relu'))\n",
    "        net.add(gluon.nn.Flatten())\n",
    "        net.add(gluon.nn.Dense(1164))\n",
    "        net.add(gluon.nn.Activation('relu'))\n",
    "        net.add(gluon.nn.Dense(100))\n",
    "        net.add(gluon.nn.Activation('relu'))\n",
    "        net.add(gluon.nn.Dense(50))\n",
    "        net.add(gluon.nn.Activation('relu'))\n",
    "        net.add(gluon.nn.Dense(10))\n",
    "        net.add(gluon.nn.Activation('relu'))\n",
    "        net.add(gluon.nn.Dense(1))\n",
    "    net.hybridize()\n",
    "    return net\n",
    "\n",
    "def transform(x, y):\n",
    "    \"\"\"\n",
    "    Reshape the numpy arrays as 4D Tensors for MXNet.\n",
    "    \n",
    "    Arguments:\n",
    "    x -- Numpy Array of input images\n",
    "    y -- Numpy Array of labels\n",
    "    \n",
    "    Returns:\n",
    "    x -- Numpy Array as (NCHW).\n",
    "    y -- Label as Column vector.\n",
    "    \"\"\"\n",
    "    data  = x.reshape(-1, 3, 66, 200)\n",
    "    label = y.reshape(-1, 1)\n",
    "    return data, label\n",
    "\n",
    "def load_data(f_path):\n",
    "    \"\"\"\n",
    "    Retrieves and loads the training/testing data from S3.\n",
    "    \n",
    "    Arguments:\n",
    "    f_path -- Location for the training/testing input dataset.\n",
    "    \n",
    "    Returns:\n",
    "    Pre-processed training and testing data along with training and testing labels.\n",
    "    \"\"\"\n",
    "    train_x = np.load(f_path+'/train_X.npy')\n",
    "    train_y = np.load(f_path+'/train_Y.npy')\n",
    "    train_X, train_Y = transform(train_x, train_y)\n",
    "    test_x = np.load(f_path+'/valid_X.npy')\n",
    "    test_y = np.load(f_path+'/valid_Y.npy')\n",
    "    test_X, test_Y = transform(test_x, test_y)\n",
    "    return train_X, train_Y, test_X, test_Y\n",
    "\n",
    "\n",
    "def save(net, model_dir):\n",
    "    \"\"\"\n",
    "    Saves the trained model to S3.\n",
    "    \n",
    "    Arguments:\n",
    "    model -- The model returned from the `train()` function.\n",
    "    model_dir -- The model directory location to save the model.\n",
    "    \"\"\"\n",
    "    print(\"Saving the trained model ...\")\n",
    "    y = net(mx.sym.var('data'))\n",
    "    y.save('%s/model.json' % model_dir)\n",
    "    net.collect_params().save('%s/model.params' % model_dir)\n",
    "\n",
    "def accuracy(data_iterator, net, ctx):\n",
    "    \"\"\"\n",
    "    Evaluates the Accuracy of the model against the Training or Testing iterator.\n",
    "    \n",
    "    Arguments:\n",
    "    data_iterator -- Iterator.\n",
    "    net -- Gluon Model.\n",
    "    \n",
    "    Returns:\n",
    "    Accuracy of the model against the data iterator.\n",
    "    \"\"\"\n",
    "    acc = mx.metric.Accuracy()\n",
    "    for i, (data, label) in enumerate(data_iterator):\n",
    "        data = data.as_in_context(ctx)\n",
    "        label = label.as_in_context(ctx)\n",
    "        output = net(data)\n",
    "        predictions = nd.argmax(output, axis=1)\n",
    "        acc.update(preds=predictions, labels=label)\n",
    "    return acc.get()[1]\n",
    "\n",
    "# ---------------------------------------------------------------------------- #\n",
    "#                           Hosting functions                                  #\n",
    "# ---------------------------------------------------------------------------- #\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    \"\"\"\n",
    "    Load the Gluon model for hosting.\n",
    "\n",
    "    Arguments:\n",
    "    model_dir -- SageMaker model directory.\n",
    "\n",
    "    Retuns:\n",
    "    Gluon model\n",
    "    \"\"\"\n",
    "    # Load the saved Gluon model\n",
    "    symbol = mx.sym.load('%s/model.json' % model_dir)\n",
    "    outputs = mx.sym.sigmoid(data=symbol, name='sigmoid_label')\n",
    "    inputs = mx.sym.var('data')\n",
    "    param_dict = gluon.ParameterDict('model_')\n",
    "    net = gluon.SymbolBlock(outputs, inputs, param_dict)\n",
    "    net.load_params('%s/model.params' % model_dir, ctx=mx.cpu())\n",
    "    return net\n",
    "\n",
    "def transform_fn(net, data, input_content_type, output_content_type):\n",
    "    \"\"\"\n",
    "    Transform input data into prediction result.\n",
    "\n",
    "    Argument:\n",
    "    net -- Gluon model loaded from `model_fn()` function.\n",
    "    data -- Input data from the `InvokeEndpoint` request.\n",
    "    input_content_type -- Content type of the request (JSON).\n",
    "    output_content_type -- Desired content type (JSON) of the repsonse.\n",
    "    \n",
    "    Returns:\n",
    "    JSON payload of the prediction result and content type.\n",
    "    \"\"\"\n",
    "    # Parse the data\n",
    "    parsed = loads(data)\n",
    "    # Convert input to MXNet NDArray\n",
    "    nda = mx.nd.array(parsed)\n",
    "    output = net(nda)\n",
    "    prediction = nd.argmax(output, axis=1)\n",
    "    response_body = dumps(prediction.asnumpy().tolist()[0])\n",
    "    return response_body, output_content_type\n",
    "```\n",
    "</p></details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training on SageMaker\n",
    "__blah blah blah; SageMaker is cool in that it automatically detects GPU's and distributed training etc.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure SageMaker\n",
    "import boto3\n",
    "import sagemaker\n",
    "import warnings\n",
    "from sagemaker.mxnet import MXNet\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__blah blah blah; Data created in modeil 0 that's already saved as numpy arrays__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the Training and Testing Data to S3\n",
    "input_data = sagemaker_session.upload_data(path='/tmp/data', key_prefix='training_data')\n",
    "bucket = input_data.split('/')[2]\n",
    "print(\"S3 Bucket Name: {}\".format(bucket))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a MXNet Estimator\n",
    "mxnet_estimator = MXNet(\n",
    "    'model.py',\n",
    "    role=role,\n",
    "    train_instance_count=1,\n",
    "    train_instance_type='ml.p3.8xlarge',\n",
    "    output_path='s3://'+bucket,\n",
    "    hyperparameters={\n",
    "        'epochs': 12,\n",
    "        'optmizer': 'adam',\n",
    "        'learning_rate': 0.001,\n",
    "        'batch_size': 64\n",
    "    }\n",
    ")\n",
    "\n",
    "# Run the Estimator\n",
    "mxnet_estimator.fit(input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Next Steps: Hyperparameter Tuning in SageMaker\n",
    "[Module 3](../3_SageMakerHPO/README.md)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
