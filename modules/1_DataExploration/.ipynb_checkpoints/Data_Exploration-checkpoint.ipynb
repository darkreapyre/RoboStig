{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 1 - Data Exploration\n",
    "## Overview\n",
    "In this module you will explore the driving observations captured from the simulator. In order to create the most effective model for __RoboStig__ to use, exploring and understanding the data is crucial. Therefore, after exploring the data, you will determine the best features to extract and train on. Additionally, you will look at various options to transform and augment the data to ensure:\n",
    "\n",
    "1. You have a sufficient amount of data for training.\n",
    "2. The data is formatted correctly and \"cleaned\" for training.\n",
    "3. The training samples provide a enough variation to ensure that the eventual model does not overfit the training samples.\n",
    "\n",
    "This process will leverage the following packages:\n",
    "- The [zipfile](https://docs.python.org/3/library/zipfile.html) module provides tools to create, read, write, append, and list a ZIP file.\n",
    "- The [os](https://docs.python.org/3/library/os.html) module provides a portable way to interact with the operating system.\n",
    "- [urllib](https://docs.python.org/3/library/urllib.request.html#module-urllib.request) provides functions that help in opening URLs.\n",
    "- The [sagemaker](https://github.com/aws/sagemaker-python-sdk) SDK is used for training and deploying machine learning models on Amazon SageMaker.\n",
    "- [numpy](http://www.numpy.org/) is the main package for scientific computing with Python.\n",
    "- The [pandas](https://pandas.pydata.org/index.html) analysis library is an easy-to-use data structures and data analysis tool.\n",
    "- [seaborn](https://seaborn.pydata.org/) is a Python visualization library based on [matplotlib](https://matplotlib.org/).\n",
    "- [sklearn](http://scikit-learn.org/stable/index.html) is tool for data mining and data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the required libraries\n",
    "import warnings\n",
    "import zipfile\n",
    "import os\n",
    "import cv2\n",
    "import urllib.request\n",
    "import sagemaker\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from sklearn.model_selection import train_test_split\n",
    "warnings.simplefilter('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Training Obersavations\n",
    "For the sake of brevity and conformity across the various modules, a [sample dataset](https://d17h27t6h515a5.cloudfront.net/topher/2016/December/584f6edd_data/data.zip) of driving observations is already provided for you to train the __RoboStig__ model. You are welcome to create your own training datasets by running the simulator in *Training Mode* and capturing your driving behavior.\n",
    "\n",
    "<div class=\"alert alert-danger\">\n",
    "<strong>Please Note: </strong>Should you choose to use your own driving dataset, you will need to adjust the Jupyter Notebook code cells accordingly as the cells below have been adapted to the Sample dataset.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function\n",
    "def download(url):\n",
    "    \"\"\"\n",
    "    Helper function to download individual file from given url.\n",
    "    \n",
    "    Arguments:\n",
    "    url -- full URL of the file to download\n",
    "    \n",
    "    Returns:\n",
    "    filename -- downloaded file name\n",
    "    \"\"\"\n",
    "    filename = url.split(\"/\")[-1]\n",
    "    if not os.path.exists(filename):\n",
    "        urllib.request.urlretrieve(url, filename)\n",
    "    return filename\n",
    "\n",
    "# To download and extract Sample Data\n",
    "file = download('https://d17h27t6h515a5.cloudfront.net/topher/2016/December/584f6edd_data/data.zip')\n",
    "\n",
    "# Extract the file\n",
    "with zipfile.ZipFile(file) as zf:\n",
    "    zf.extractall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View the Data\n",
    "A full log of the driving observations is recorded in the `driving_log.csv` file. This file also includes the images captured from the *Left*, *Center* and *Right* camera angles from the vehicle. You will leverage this log, as well as the images, to train your model. But first, it's a good idea to view the contents of this file and derive some insights.\n",
    "\n",
    "In the following code cells, we use the `pandas` library to create a tabular format (DataFrame) and display the first five rows of the `driving_log.csv` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the log data\n",
    "data_df = pd.read_csv('./data/driving_log.csv')\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Describe the Data\n",
    "Since the `.csv` file is now loaded as a `pandas` DataFrame, we can leverage the library to perform descriptive statistics, using the `describe()` method as the following code cell shows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Overview\n",
    "print(\"Dataset Shape: {}\\n\".format(data_df.shape))\n",
    "print(data_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the data, you will notice that there are $8036$ observations or driving \"log\" entries, which will be used to train your __RoboStig__ model. You can also see that there are $7$ features (or columns) to classify or categorize the observations.\n",
    "\n",
    "By describing the summary statistics of the DataFrame, there are some interesting details that are further highlighted, namely:\n",
    "\n",
    "1. Two thirds of the *steering* observations are $0.0$. This highlightes that the majority of the observations have a $0^o$ steering angle. In other words, the majority of the observations record the vehicle driving straight.\n",
    "2. Two thirds of the *speed* observations clock the vehicle driving at around $30 Mph$. This coincides with the *max* speed value.\n",
    "3. There is a correlation between the *speed* and *brake* observations since two thirds of the data shows that there is zero braking and and the average *speed* is around $28.1 Mph$.\n",
    "\n",
    "## Visualize the Data\n",
    "Another tactic to better understand the data, it to visualize some of the summary statistics. Once of the most common visualization techniques is to view the distribution or __shape__ of the data, such as a histogram. \n",
    "\n",
    "A well-known distribution of continuous values is the __bell curve__, as known as the __“normal”__ distribution or __\"Gaussian\"__ distribution. (Named after [Carl Friedrich Gauss](https://en.wikipedia.org/wiki/Carl_Friedrich_Gauss), first described it).\n",
    "\n",
    "Since the object for __RoboStig__ is to steer the vehicle around the track, a good place to start visualizing the distrubtion of the *steering* angles. The following code cell shows the distribution of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the distribution of the data\n",
    "# using the `seabourne` library\n",
    "sns.set(rc={'figure.figsize':(10, 10)})\n",
    "fig = sns.distplot(data_df.steering)\n",
    "plt.xlabel(\"Steering Angle\")\n",
    "plt.title(\"Distribution of Steering Angles in the Dataset\")\n",
    "plt.show(fig);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can visualize from the visualization above, the sample dataset is skewed toward the zero steering angle. In essence, any machine learning model trained from this data will only really know how to drive in a straight line. \n",
    "\n",
    "__Somehow you will need to get more training data that provides a better spread of steering angles!__\n",
    "\n",
    "The next section will show you how to create more data from the existing dataset by preprocessing it.\n",
    "\n",
    "---\n",
    "# Data Preprocessing\n",
    "One of the most important steps in any Machine Learning Pipeline is the *Data Preprocessing* step. This is where you ensure that you have the right data, that the data is correctly formatted and, most importantly, the data usable for training. The typical tasks that can take place during this step are:\n",
    "\n",
    "- Removing or inferring missing data.\n",
    "- Extracting the needed features from the data and removing unnecessary data..\n",
    "- Inferring or creating more data.\n",
    "\n",
    "In this next section, you will be implementing some of these tasks.\n",
    "\n",
    "## Feature Extraction\n",
    "As already mentioned, the *steering* angle is what __RoboStig__ will use to drive the car, thus it is the __outcome__ variable that your model is going to predict. The *independent* variables in this scenario, sometimes called the __experimental__ or __predictor__ variables are the *throttle*; *brake*; *speed* as well as all the images from the images for the *center*, *left* and *right* camera angles. \n",
    "\n",
    "So the first step in preprocessing the data is to separate the *outcome* and *predictor* variables from the dataset. Since the the variables are already loaded as `pandas dataframe`, the following cell shows how to separate the features into the specific category. The result will be the *predictor* features as `X` and the outcome as `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the features\n",
    "X = data_df[['center', 'left', 'right']].values\n",
    "y = data_df['steering'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Transformations\n",
    "Along with extracting the necessary features from the data, a similar pre-processing steps need to be applied to the images images themselves. This is necessary to align with the image formatting requirements of the NVIVIA model.\n",
    "\n",
    "<div class=\"alert alert-danger\">\n",
    "<strong>Please Note: </strong>Every Code Cell has the solution that you can use if you get stuck. Simply click on the <strong>(Click to expand)</strong> link below each Code Cell.\n",
    "</div>\n",
    "\n",
    "__Exercise:__ For your first exercise, you will need to implement the following image transformation functions:\n",
    "\n",
    "1. __Crop images to only focus on the Region of Interest (ROI)__. The ROI in this case is simply the road. Therefore you will want to remove unnecessary parts of the image that are not relevant to the specific features that you want the model to focus on. So in this case, you will want to crop out the Sky as well as the hood of the vehicle.\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "<strong>Hint: </strong>The input images from the simulator are in RGB format and represented as a `numpy.array` of format (Height, Width, Channel. Extract just the parts of the height array and width array for the ROI.\n",
    "</div>\n",
    "\n",
    "2. __Resize images__. The NVIDIA model uses input images that are shaped as $66 \\times 200 \\times 3$, where the *height* $ = 66$ pixels, the *width* $= 200$ pixels and image *channels* $= 3$ (for *Red*, *Green* and *Blue*). \n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "<strong>Info: </strong>This function has laready been implemented for you.\n",
    "</div>\n",
    "\n",
    "3. __Convert to YUV Channels__. The NVIDIA model is trained with the images converted from RGB to YUV color encoding. You will need to implement this transformation too.\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "<strong>Hint: </strong>The `cv2.cvtColor()` function from the OpenCV library may be helpful.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Transformation: Crop\n",
    "def crop(image):\n",
    "    \"\"\"\n",
    "    Crops the image by emoving the sky at the top and the car front at the bottom.\n",
    "\n",
    "    Arguments:\n",
    "    image -- numpy.array representing an RGB image of format (Height, Width, Channel).\n",
    "   \n",
    "    Returns:\n",
    "    Cropped image.\n",
    "    \"\"\"\n",
    "\n",
    "    ### START OF YOUR CODE (≈1 line of code) ###\n",
    "    image = None\n",
    "    ### END OF YOUR CODE ###\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary><b>Solution (Click to expand)</b></summary><p>\n",
    "\n",
    "```\n",
    "# Image Transformation: Crop\n",
    "def crop(image):\n",
    "    \"\"\"\n",
    "    Crops the image by emoving the sky at the top and the car front at the bottom.\n",
    "\n",
    "    Arguments:\n",
    "    image -- numpy.array representing an RGB image of format (Height, Width, Channel).\n",
    "   \n",
    "    Returns:\n",
    "    Cropped image.\n",
    "    \"\"\"\n",
    "    image = image[60:-25, :, :]\n",
    "\n",
    "    return image\n",
    "```\n",
    "\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Transformation: Resize\n",
    "def resize(image):\n",
    "    \"\"\"\n",
    "    Resize the image to the input shape for the NVIDIA model.\n",
    "    \n",
    "    Note: The parameters `IMAGE_WIDTH` and `IMAGE_HEIGHT` are\n",
    "          declared as part of a pre-processing pipeline.\n",
    "\n",
    "    Arguments:\n",
    "    image -- numpy array representing image.\n",
    "\n",
    "    Returns:\n",
    "    Resized image.\n",
    "    \"\"\"\n",
    "    image = cv2.resize(image, (IMAGE_WIDTH, IMAGE_HEIGHT), cv2.INTER_AREA)\n",
    "\n",
    "    return image\n",
    "\n",
    "# Image Transformation: Convert from RGB to YUV\n",
    "def rgb2yuv(image):\n",
    "    \"\"\"\n",
    "    Convert the image from RGB to YUV color space.\n",
    "\n",
    "    Arguments:\n",
    "    image -- numpy array represnting the image.\n",
    "\n",
    "    Returns:\n",
    "    YUV image.\n",
    "    \"\"\"\n",
    "\n",
    "    ### START OF YOUR CODE  (≈1 line of code) ###\n",
    "    image = None\n",
    "    ### END OF YOUR CODE ###\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary><b>Solution (Click to expand)</b></summary><p>\n",
    "\n",
    "```\n",
    "# Image Transformation: Convert from RGB to YUV\n",
    "def rgb2yuv(image):\n",
    "    \"\"\"\n",
    "    Convert the image from RGB to YUV color space.\n",
    "\n",
    "    Arguments:\n",
    "    image -- numpy array represnting the image.\n",
    "\n",
    "    Returns:\n",
    "    YUV image.\n",
    "    \"\"\"\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2YUV)\n",
    "\n",
    "    return image\n",
    "```\n",
    "\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Augmentation\n",
    "As already mentioned, you will need to add more diversity to the Training data, so the data isn;t skewed toward only driving straight. In order to accomplish this, you will take some of the existing images (primarily the __left__ and __right__ camera images) and adjust them just enough so that they can add the additional variance required.\n",
    "\n",
    "<div class=\"alert alert-primary\" role=\"alert\">\n",
    "<strong>Info: </stong>Many of the Python Machine Learning framework have built in methods to augment images. You can leverasge these if you wish, <strong>BUT</strong> make sure to adjust the steering angle data accordingly.\n",
    "</div>\n",
    "\n",
    "__Exercise:__ For the next exercise, you will implement *some* of the image augmentation techniques.\n",
    "\n",
    "1. __Randomly flip the images.__ Here you will want to take $50%$ of all images and randomly flip them vertically. If the image is flipped, then the reverse of the steering angle must be applied.\n",
    "\n",
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "<strong>Hint: </strong>The `cv2.flip()` function may be helpful.\n",
    "</div>\n",
    "\n",
    "2. __Randomly shift the images.__ Here the image is shifted vertically and horizontally by a $0.002^O$ pixel shift. The steering angle is adjusted accordingly.\n",
    "\n",
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "<strong>Info: </strong>This function has already been implemented for you.\n",
    "</div>\n",
    "\n",
    "3. __Randomly distort the images.__ This augmentation technique applies a random distortion to the the images and also randomly adjusts the brightness of the images.\n",
    "\n",
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "<strong>Info: </strong>This function has already been implemented for you.\n",
    "</div>\n",
    "\n",
    "4. __Randomly adjust the brightness of the images.__ Here you will implement a function adjust the brightness, either making the image brighter or lowering the brightness (making the image darker).\n",
    "\n",
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "<strong>Hint: </strong>The `cv2.cvtColor()` function may be helpful, paying attention to HSV (Hue, Saturation, Value) or HSB.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Augmentation: Random Flip\n",
    "def random_flip(image, steering_angle):\n",
    "    \"\"\"\n",
    "    Randomly - 50% of the time - flip the image left from left to right and vice-versa. \n",
    "    Additionally, adjust the steering angle accordingly.\n",
    "\n",
    "    Arguments:\n",
    "    image -- pre-processed input image.\n",
    "    steering_amngle -- pre-processed steering angle.\n",
    "\n",
    "    Returns:\n",
    "    image -- flipped image.\n",
    "    steering_angle -- adjusted steering angle.\n",
    "    \"\"\"\n",
    "    ### START OF YOUR CODE (≈3 lines of code) ###\n",
    "\n",
    "    return None\n",
    "    ### END OF YOUR CODE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary><b>Solution (Click to expand)</b></summary><p>\n",
    "\n",
    "```\n",
    "# Image Augmentation: Random Flip\n",
    "def random_flip(image, steering_angle):\n",
    "    \"\"\"\n",
    "    Randomly flip the 50% of the images from left to right and vice-versa.\n",
    "    Additionally, adjust the steering angle accordingly.\n",
    "\n",
    "    Arguments:\n",
    "    image -- pre-processed input image.\n",
    "    steering_amngle -- pre-processed steering angle.\n",
    "\n",
    "    Returns:\n",
    "    image -- flipped image.\n",
    "    steering_angle -- adjusted steering angle.\n",
    "    \"\"\"\n",
    "    # Randomly select 0.5 of the images\n",
    "    if np.random.rand() < 0.5:\n",
    "        # Apply the flip function to the vertical axis.\n",
    "        image = cv2.flip(image, 1)\n",
    "\n",
    "        # Adjust the steering angle to the reverse of the current steering angle.\n",
    "        steering_angle = -steering_angle\n",
    "\n",
    "    # Return the \"flipped\" image and new steering angle.\n",
    "\n",
    "    return image, steering_angle\n",
    "```\n",
    "\n",
    "</p><details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Augmentation: Random Translate\n",
    "def translate(image, steering_angle, x_range, y_range):\n",
    "    \"\"\"\n",
    "    Randomly shift (translate) the image vertically and horizontally.\n",
    "\n",
    "    Arguments:\n",
    "    image -- pre-processed input image.\n",
    "    steering_angle -- pre-processed steering angle.\n",
    "    x_range -- x-axis pixels.\n",
    "    y_range -- y-axis pixels.\n",
    "\n",
    "    Returns:\n",
    "    image -- translated image.\n",
    "    steering_angle -- adjusted steeing angle.\n",
    "    \"\"\"\n",
    "    # Randomly adjust the x and y axis\n",
    "    x_transform = x_range * (np.random.rand() - 0.5)\n",
    "    y_transform = y_range * (np.random.rand() - 0.5)\n",
    "\n",
    "    # Adjust the steering angle\n",
    "    steering_angle += x_transform * 0.002\n",
    "    m_transform = np.float32([[1, 0, x_transform], [0, 1, y_transform]])\n",
    "    height, width = image.shape[:2]\n",
    "    image = cv2.warpAffine(image, m_transform, (width, height))\n",
    "\n",
    "    return image, steering_angle\n",
    "\n",
    "# Image Augmentation: Random Distortion\n",
    "def distort(image):\n",
    "    \"\"\"\n",
    "    Add distortion to random images and adjust the brightness.\n",
    "\n",
    "    Arguments:\n",
    "    image -- pre-processed input image.\n",
    "\n",
    "    Returns:\n",
    "    new_image -- distorted image.\n",
    "    \"\"\"\n",
    "    # Create placeholder numpy array for the new image\n",
    "    new_img = image.astype(float)\n",
    "\n",
    "    # Add random brightness\n",
    "    value = np.random.randint(-28, 28)\n",
    "    if value > 0:\n",
    "        mask = (new_img[:, :, 0] + value) > 255\n",
    "    if value <= 0:\n",
    "        mask = (new_img[:, :, 0] + value) < 0\n",
    "    new_img[:,:,0] += np.where(mask, 0, value)\n",
    "\n",
    "    # Add random shadow \n",
    "    h,w = new_img.shape[0:2]\n",
    "    mid = np.random.randint(0, w)\n",
    "    factor = np.random.uniform(0.6, 0.8)\n",
    "    if np.random.rand() > .5:\n",
    "        new_img[:, 0:mid, 0] *= factor\n",
    "    else:\n",
    "        new_img[:, mid:w, 0] *= factor\n",
    "    \n",
    "    # Randomly shift the horizon\n",
    "    h, w, _ = new_img.shape\n",
    "    horizon = 2 * h / 5\n",
    "    v_shift = np.random.randint(-h / 8, h / 8)\n",
    "    pts1 = np.float32([[0, horizon], [w, horizon], [0, h], [w, h]])\n",
    "    pts2 = np.float32([[0, horizon + v_shift], [w, horizon + v_shift], [0, h], [w, h]])\n",
    "    M = cv2.getPerspectiveTransform(pts1,pts2)\n",
    "    new_img = cv2.warpPerspective(new_img, M, (w, h), borderMode=cv2.BORDER_REPLICATE)\n",
    "\n",
    "    return new_img.astype(np.uint8)\n",
    "\n",
    "# Image Augmentation: Random Brightness\n",
    "def brightness(image):\n",
    "    \"\"\"\n",
    "    Randomly adjust brightness of the image.\n",
    "\n",
    "    Arguments:\n",
    "    image -- pro-processed input image.\n",
    "\n",
    "    Returns:\n",
    "    HSV/HSB converted image.\n",
    "    \"\"\"\n",
    "    ### START OF YOUR CODE (≈4 lines of code) ###\n",
    "    \n",
    "    return None\n",
    "    ### END OF YOUR CODE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary><b>Solution (Click to expand)</b></summary><p>\n",
    "\n",
    "```\n",
    "# Image Augmentation: Random Brightness\n",
    "def brightness(image):\n",
    "    \"\"\"\n",
    "    Randomly adjust brightness of the image.\n",
    "    \"\"\"\n",
    "    # Convert image to HSV\n",
    "    hsv = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\n",
    "\n",
    "    # Randomly adjust the brightness ratio and apply it\n",
    "    # to the image\n",
    "    ratio = 1.0 + 0.4 * (np.random.rand() - 0.5)\n",
    "    hsv[:,:,2] =  hsv[:,:,2] * ratio\n",
    "\n",
    "    # Convert back to RGB and return the image\n",
    "    return cv2.cvtColor(hsv, cv2.COLOR_HSV2RGB)\n",
    "```\n",
    "\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Augmentation Example\n",
    "\n",
    "```text\n",
    "|--data\n",
    "|  |--driving_log.csv\n",
    "|  |--IMG\n",
    "|  |  |--center_2016_12_01_13_30_48_287.jpg\n",
    "|  |  |--left_2016_12_01_13_30_48_287.jpg\n",
    "|  |  ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function\n",
    "def load(data_dir, image_file):\n",
    "    \"\"\"\n",
    "    Load RGB images from a file\n",
    "    \"\"\"\n",
    "    return mpimg.imread(os.path.join(data_dir, image_file.strip()))\n",
    "\n",
    "def transform(image):\n",
    "    \"\"\"\n",
    "    Combine all preprocess functions into one\n",
    "    \"\"\"\n",
    "    image = crop(image)\n",
    "    image = resize(image)\n",
    "    image = rgb2yuv(image)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Sample Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Origional 'left' image\n",
    "IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNELS = 66, 200, 3\n",
    "INPUT_SHAPE = (IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNELS)\n",
    "random_image = X[100][0]\n",
    "img = load('data', random_image)\n",
    "#img = ('', random_image)\n",
    "plt.rcParams['figure.figsize'] = (11.0, 10.0)\n",
    "plt.imshow(img);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Augmented Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Subpluts for Augmented Images\n",
    "plt.close('all')\n",
    "fig = plt.figure(figsize=(18., 10.))\n",
    "sub1 = fig.add_subplot(221)\n",
    "sub1.set_title('Bright')\n",
    "sub1.imshow(brightness(img))\n",
    "sub2 = fig.add_subplot(222)\n",
    "sub2.set_title('Horizontal Flip')\n",
    "sub2.imshow(cv2.flip(img, 1))\n",
    "sub3 = fig.add_subplot(223)\n",
    "sub3.set_title('Random Distortion')\n",
    "sub3.imshow(distort(img));\n",
    "sub4 = fig.add_subplot(224)\n",
    "sub4.set_title('Final Image')\n",
    "sub4.imshow(transform(img));\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Augmentation Pipeline...\n",
    "__BLah Blah Blah__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch Image Configurations\n",
    "HEIGHT, WIDTH, CHANNELS = 66, 200, 3\n",
    "INPUT_SHAPE = (HEIGHT, WIDTH, CHANNELS)\n",
    "\n",
    "# Aumentation Pipeline Functions\n",
    "def choose(data_dir, center, left, right, steering_angle):\n",
    "    \"\"\"\n",
    "    Randomly choose an image from the center, left or right, and adjust\n",
    "    the steering angle.\n",
    "    \"\"\"\n",
    "    choice = np.random.choice(3)\n",
    "    if choice == 0:\n",
    "        return load(data_dir, left), steering_angle + 0.2\n",
    "    elif choice == 1:\n",
    "        return load(data_dir, right), steering_angle - 0.2\n",
    "    return load(data_dir, center), steering_angle\n",
    "\n",
    "def augment(data_dir, center, left, right, steering_angle, range_x=100, range_y=10):\n",
    "    \"\"\"\n",
    "    Generate an augumented image and adjust steering angle.\n",
    "    (The steering angle is associated with the center image)\n",
    "    \"\"\"\n",
    "    image, steering_angle = choose(data_dir, center, left, right, steering_angle)\n",
    "    image, steering_angle = random_flip(image, steering_angle)\n",
    "    image, steering_angle = translate(image, steering_angle, range_x, range_y)\n",
    "    image = brightness(image)\n",
    "    image = distort(image)\n",
    "    return image, steering_angle\n",
    "\n",
    "def aug_pipeline(data_dir, image_paths, steering_angles, is_training):\n",
    "    # Generate training image given image paths and associated steering angles\n",
    "    # Create numpy array to store augmented images\n",
    "    images = np.empty([image_paths.shape[0], HEIGHT, WIDTH, CHANNELS])\n",
    "    steering = np.empty(images_paths.shape[0])\n",
    "    while True:\n",
    "        i = 0\n",
    "        for index in np.random.permutation(image_paths.shape[0]):\n",
    "            center, left, right = image_paths[index]\n",
    "            steering_angle = steering_angles[index]\n",
    "            # Random augmentation for training data\n",
    "            if is_training and np.random.rand() < 1.:\n",
    "                # Augment all the randomly selected images\n",
    "                image, steering_angle = augment(data_dir, center, left, right, steering_angle)\n",
    "            else:\n",
    "                # Load only the `center` image\n",
    "                image = load(data_dir, center)\n",
    "            # Transform and add the image with steering angle to the placeholder numpy array\n",
    "            images[i] = transform(image)\n",
    "            steering[i] = steering_angle\n",
    "            i += 1\n",
    "            # Return placeholder numpy arrays\n",
    "            return np.array(images).astype(np.float32), np.array(steering).astype(np.float32)\n",
    "\n",
    "\"\"\"\n",
    "def aug_pipeline(data_dir, image_paths, steering_angles, batch_size, is_training):\n",
    "    \"\"\"\n",
    "    Generate training image give image paths and associated steering angles\n",
    "    \"\"\"\n",
    "    images = np.empty([batch_size, HEIGHT, WIDTH, CHANNELS])\n",
    "    steering = np.empty(batch_size)\n",
    "    while True:\n",
    "        i = 0\n",
    "        for index in np.random.permutation(image_paths.shape[0]):\n",
    "            center, left, right = image_paths[index]\n",
    "            steering_angle = steering_angles[index]\n",
    "            # argumentation\n",
    "            if is_training and np.random.rand() < 1.:\n",
    "                image, steering_angle = augument(data_dir, center, left, right, steering_angle)\n",
    "            else:\n",
    "                image = load(data_dir, center) \n",
    "            # add the image and steering angle to the batch\n",
    "            images[i] = transform(image)\n",
    "            steering[i] = steering_angle\n",
    "            i += 1\n",
    "            if i == batch_size:\n",
    "                break\n",
    "        return np.array(images).astype(np.float32), np.array(steering).astype(np.float32)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the Augmentation Pipeline functions to create the Training and Vlaidation Datasets using a 90/10 split respectivley. There is no Test Dataset as the final test will be acomplished using the Simulator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sample, y_sample = aug_pipeline('data', X, y, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot New Distribution of training examples\n",
    "fig = sns.distplot(y_sample)\n",
    "plt.xlabel(\"Steering Angle\")\n",
    "plt.title(\"New Distribution of Steering Angles in the Training Dataset\")\n",
    "plt.show(fig);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen from the sample datatset, the disribution of steering angles is more uniform. Next we create the datasets for the model and upload them to S3.\n",
    "\n",
    "---\n",
    "# Prepare the Training/Validation Datsets\n",
    "Using a 90/10 split, the dataset is separated to 90% for Training and 10% for Validation. The final test onm the datset will be handled by the simulator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries and SageMaker configuration\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "# Create Training and Validation datasets\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__blah blah blah__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess through the pipline\n",
    "X_train, y_train = aug_pipeline('data', X_train, y_train, True)\n",
    "X_valid, y_valid = aug_pipeline('data', X_valid, y_valid, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__blah blah blah__  \n",
    "\n",
    "<details><summary>NOTE TO SELF</summary>\n",
    "<p>\n",
    "Need to double check if reshaping is necessary as `model.py` includes a `tranform()` function.  \n",
    "</p>\n",
    "<p>\n",
    "```\n",
    "# Reshape images as a 4D Tensor\n",
    "X_train = X_train.reshape(-1, 3, 66, 200)\n",
    "y_train = y_train.reshape(-1, 1)\n",
    "X_valid = X_valid.reshape(-1, 3, 66, 200)\n",
    "y_valid = y_valid.reshape(-1, 1)\n",
    "```\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View resultant shape\n",
    "print(\"Training Dataset Shape: {}\\n\".format(X_train.shape))\n",
    "print(\"Validation Dataset Shape: {}\".format(X_valid.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__blah blah blah__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create local repository for Numpy Arrays\n",
    "if not os.path.exists('/tmp/data'):\n",
    "    os.mkdir('/tmp/data')\n",
    "\n",
    "# Save the Dataset as Numpy Arrays\n",
    "np.save('/tmp/data/train_X.npy', X_train)\n",
    "np.save('/tmp/data/train_Y.npy', y_train)\n",
    "np.save('/tmp/data/valid_X.npy', X_valid)\n",
    "np.save('/tmp/data/valid_Y.npy', y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Next Step: Build and Train the model in SageMaker\n",
    "[Module 2](../2_SageMakerBYOM/README.md)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
