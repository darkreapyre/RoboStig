#!/usr/bin/env python

# Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.

# Applied from https://github.com/awslabs/amazon-sagemaker-examples/tree/master/hyperparameter_tuning/keras_bring_your_own
from __future__ import absolute_import
from __future__ import print_function

# Import libraries necessary for this project.
import json
import os
import sys
import numpy as np
import pandas as pd
from keras.models import Sequential
from keras.layers import Activation, Conv2D, Dense, Dropout, ELU, Flatten
from keras.optimizers import Adam
from PIL import Image
from sklearn.model_selection import train_test_split
from environment import create_trainer_environment

# Configure the trainer environemnt for SageMaker training
env = create_trainer_environment()
print('creating SageMaker trainer environment:\n%s' % str(env))

# Get hyperparameters
batch_size = env.hyperparameters.get('batch_size', default=64, object_type=int)
learning_rate = env.hyperparameters.get('learning_rate', default=.001, object_type=float)
EPOCHS = env.hyperparameters.get('epochs', default=25, object_type=int)

# Location of the simulator data.
data_dir = os.path.join(env.channel_dirs['train'], 'data')

# Define the model
def build_model():
    model = Sequential()
    model.add(Conv2D(3, 1, 1, input_shape=(90, 320, 3)))
    model.add(ELU())
    model.add(Dropout(0.5))
    model.add(Conv2D(3, 5, 5, subsample=(2, 2)))
    model.add(ELU())
    model.add(Dropout(0.5))
    model.add(Conv2D(24, 5, 5, subsample=(2, 2)))
    model.add(ELU())
    model.add(Dropout(0.5))
    model.add(Conv2D(36, 5, 5, subsample=(2, 2)))
    model.add(ELU())
    model.add(Dropout(0.5))
    model.add(Conv2D(48, 3, 3))
    model.add(ELU())
    model.add(Dropout(0.5))
    model.add(Conv2D(64, 3, 3))
    model.add(ELU())
    model.add(Dropout(0.5))
    model.add(Conv2D(128, 3, 3))
    model.add(ELU())
    model.add(Dropout(0.5))
    model.add(Flatten())
    model.add(Dense(100))
    model.add(ELU())
    model.add(Dropout(0.5))
    model.add(Dense(50))
    model.add(ELU())
    model.add(Dropout(0.5))
    model.add(Dense(10))
    model.add(ELU())
    model.add(Dropout(0.5))
    model.add(Dense(1))
    model.summary()
    return model

# Helper to load an normalize images.
def load_image(path, flip=False):
    # Read the image from disk, and flip it if requested.
    image = Image.open(path.strip())
    if flip:
        image = image.transpose(Image.FLIP_LEFT_RIGHT)
    
    # Normalize the image pixels to range -1 to 1.
    image = np.array(image, np.float32)
    image /= 127.5
    image -= 1.
    
    # Slice off the top and bottom pixels to remove the sky
    # and the hood of the car.
    image = image[40:130, :]
    
    # Return the normalized image.
    return image

# Data generator.
def generate_batches(images, angles, batch_size, augment=True):
    # Create an array of sample indexes.
    indexes = np.arange(len(images))
    batch_images = []
    batch_angles = []
    sample_index = 0
    while True:
        # Reshuffle the indexes after each pass through the samples to minimize
        # overfitting on the data.
        np.random.shuffle(indexes)
        for i in indexes:
            # Increment the number of samples. 
            sample_index += 1
            
            # Load the center image and weight.
            center_image = load_image(images.iloc[i]['Center Image'])
            center_angle = float(angles.iloc[i])
            batch_images.append(center_image)
            batch_angles.append(center_angle)
            
            # Add augmentation if requested
            if augment:
                # Load the flipped image and invert angle
                flipped_image = load_image(images.iloc[i]['Center Image'], True)
                flipped_angle = -1. * center_angle
                batch_images.append(flipped_image)
                batch_angles.append(flipped_angle)

                # Load the left image and adjust angle
                left_image = load_image(images.iloc[i]['Left Image'])
                left_angle = min(1.0, center_angle + 0.25)
                batch_images.append(left_image)
                batch_angles.append(left_angle)
                # Load the right image and adjust angle
                right_image = load_image(images.iloc[i]['Right Image'])
                right_angle = max(-1.0, center_angle - 0.25)
                batch_images.append(right_image)
                batch_angles.append(right_angle)
            
            # If we have processed batch_size samples or this is the last batch
            # of the epoch, then submit the batch. Note that due to augmentation
            # there may be more than batch_size elements in the batch.
            if (sample_index % batch_size) == 0 or (sample_index % len(images)) == 0:
                yield np.array(batch_images), np.array(batch_angles)
                batch_images = []
                batch_angles = []

# Training function
def train():
    print("Starting model training ...")
    
    # Local variables
    DATA_FILE = os.path.join(data_dir, 'driving_log.csv')

    # Load the training data from the simulator.
    cols = ['Center Image', 'Left Image', 'Right Image', 'Steering Angle', 'Throttle', 'Break', 'Speed']
    data = pd.read_csv(DATA_FILE, names=cols, header=1)

    # Separate the image paths and steering angles.
    images = data[['Center Image', 'Left Image', 'Right Image']]
    angles = data['Steering Angle']

    # Split the data into training and validation sets.
    images_train, images_validation, angles_train, angles_validation = train_test_split(images, angles, test_size=0.15, random_state=42)
    samples_per_epoch = 4 * len(images_train)
    generator_train = generate_batches(images_train, angles_train, batch_size)
    nb_val_samples = len(images_validation)
    generator_validation = generate_batches(images_validation, angles_validation, batch_size, augment=False)

    # Run the model.
    model = build_model()
    
    # Select the optimizer and compile the model.
    optimizer = Adam(lr=learning_rate)
    model.compile(loss='mse', optimizer=optimizer)
    model.fit_generator(
        generator_train,
        samples_per_epoch=samples_per_epoch,
        nb_epoch=nb_epoch,
        validation_data=generator_validation,
        nb_val_samples=nb_val_samples
    )

    # Save the generated model and weights.
    model.save(os.path.join(model_dir, 'model.h5'))

if __name__ == '__main__':
    train()
    sys.exit(0)