{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Keras BYOC - Manual\n",
    "### Libraries for Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import shutil\n",
    "import os, shutil\n",
    "import random\n",
    "import cv2\n",
    "import math\n",
    "import json\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import sagemaker\n",
    "import glob\n",
    "\n",
    "import keras\n",
    "from keras.preprocessing.image import *\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Conv2D, Flatten, MaxPooling2D, Lambda, ELU\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import Callback, ModelCheckpoint\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.regularizers import l2\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Libraries for Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from IPython.display import display # Allows the use of display() for DataFrames\n",
    "\n",
    "# Visualizations will be shown in the notebook.\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Sample `Model.py`\n",
    "### Helper Functions - Image Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download(url):\n",
    "    \"\"\"\n",
    "    Helper function to download individual file from given url.\n",
    "    \n",
    "    Arguments:\n",
    "    url -- full URL of the file to download\n",
    "    \n",
    "    Returns:\n",
    "    filename -- downloaded file name\n",
    "    \"\"\"\n",
    "    filename = url.split(\"/\")[-1]\n",
    "    if not os.path.exists(filename):\n",
    "        urllib.request.urlretrieve(url, filename)\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(data_dir, image_file):\n",
    "    \"\"\"\n",
    "    Load RGB images from a file\n",
    "    \"\"\"\n",
    "    return mpimg.imread(os.path.join(data_dir, image_file.strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop(image):\n",
    "    \"\"\"\n",
    "    Crop the image (removing the sky at the top and the car front at the bottom)\n",
    "    \"\"\"\n",
    "    return image[60:-25, :, :] # remove the sky and the car front"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize(image):\n",
    "    \"\"\"\n",
    "    Resize the image to the input shape used by the network model\n",
    "    \"\"\"\n",
    "    return cv2.resize(image, (IMAGE_WIDTH, IMAGE_HEIGHT), cv2.INTER_AREA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rgb2yuv(image):\n",
    "    \"\"\"\n",
    "    Convert the image from RGB to YUV (This is what the NVIDIA model does)\n",
    "    \"\"\"\n",
    "    return cv2.cvtColor(image, cv2.COLOR_RGB2YUV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(image):\n",
    "    \"\"\"\n",
    "    Combine all preprocess functions into one\n",
    "    \"\"\"\n",
    "    image = crop(image)\n",
    "    image = resize(image)\n",
    "    image = rgb2yuv(image)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_image(data_dir, center, left, right, steering_angle):\n",
    "    \"\"\"\n",
    "    Randomly choose an image from the center, left or right, and adjust\n",
    "    the steering angle.\n",
    "    \"\"\"\n",
    "    choice = np.random.choice(3)\n",
    "    if choice == 0:\n",
    "        return load_image(data_dir, left), steering_angle + 0.2\n",
    "    elif choice == 1:\n",
    "        return load_image(data_dir, right), steering_angle - 0.2\n",
    "    return load_image(data_dir, center), steering_angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_flip(image, steering_angle):\n",
    "    \"\"\"\n",
    "    Randomly flipt the image left <-> right, and adjust the steering angle.\n",
    "    \"\"\"\n",
    "    if np.random.rand() < 0.5:\n",
    "        image = cv2.flip(image, 1)\n",
    "        steering_angle = -steering_angle\n",
    "    return image, steering_angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_translate(image, steering_angle, range_x, range_y):\n",
    "    \"\"\"\n",
    "    Randomly shift the image virtially and horizontally (translation).\n",
    "    \"\"\"\n",
    "    trans_x = range_x * (np.random.rand() - 0.5)\n",
    "    trans_y = range_y * (np.random.rand() - 0.5)\n",
    "    steering_angle += trans_x * 0.002\n",
    "    trans_m = np.float32([[1, 0, trans_x], [0, 1, trans_y]])\n",
    "    height, width = image.shape[:2]\n",
    "    image = cv2.warpAffine(image, trans_m, (width, height))\n",
    "    return image, steering_angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distort(image):\n",
    "    ''' \n",
    "    method for adding random distortion to dataset images, including random brightness adjust, and a random\n",
    "    vertical shift of the horizon position\n",
    "    '''\n",
    "    new_img = image.astype(float)\n",
    "    # random brightness - the mask bit keeps values from going beyond (0,255)\n",
    "    value = np.random.randint(-28, 28)\n",
    "    if value > 0:\n",
    "        mask = (new_img[:,:,0] + value) > 255 \n",
    "    if value <= 0:\n",
    "        mask = (new_img[:,:,0] + value) < 0\n",
    "    new_img[:,:,0] += np.where(mask, 0, value)\n",
    "    # random shadow - full height, random left/right side, random darkening\n",
    "    h,w = new_img.shape[0:2]\n",
    "    mid = np.random.randint(0,w)\n",
    "    factor = np.random.uniform(0.6,0.8)\n",
    "    if np.random.rand() > .5:\n",
    "        new_img[:,0:mid,0] *= factor\n",
    "    else:\n",
    "        new_img[:,mid:w,0] *= factor\n",
    "    # randomly shift horizon\n",
    "    h,w,_ = new_img.shape\n",
    "    horizon = 2*h/5\n",
    "    v_shift = np.random.randint(-h/8,h/8)\n",
    "    pts1 = np.float32([[0,horizon],[w,horizon],[0,h],[w,h]])\n",
    "    pts2 = np.float32([[0,horizon+v_shift],[w,horizon+v_shift],[0,h],[w,h]])\n",
    "    M = cv2.getPerspectiveTransform(pts1,pts2)\n",
    "    new_img = cv2.warpPerspective(new_img,M,(w,h), borderMode=cv2.BORDER_REPLICATE)\n",
    "    return new_img.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Radomly Adjust Brightness\n",
    "def random_brightness(image):\n",
    "    \"\"\"\n",
    "    Randomly adjust brightness of the image.\n",
    "    \"\"\"\n",
    "    # HSV (Hue, Saturation, Value) is also called HSB ('B' for Brightness).\n",
    "    hsv = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\n",
    "    ratio = 1.0 + 0.4 * (np.random.rand() - 0.5)\n",
    "    hsv[:,:,2] =  hsv[:,:,2] * ratio\n",
    "    return cv2.cvtColor(hsv, cv2.COLOR_HSV2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augument(data_dir, center, left, right, steering_angle, range_x=100, range_y=10):\n",
    "    \"\"\"\n",
    "    Generate an augumented image and adjust steering angle.\n",
    "    (The steering angle is associated with the center image)\n",
    "    \"\"\"\n",
    "    image, steering_angle = choose_image(data_dir, center, left, right, steering_angle)\n",
    "    image, steering_angle = random_flip(image, steering_angle)\n",
    "    image, steering_angle = random_translate(image, steering_angle, range_x, range_y)\n",
    "    image = distort(image)\n",
    "    image = random_brightness(image)\n",
    "    return image, steering_angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(data_dir, image_paths, steering_angles, batch_size, is_training):\n",
    "    \"\"\"\n",
    "    Generate training image give image paths and associated steering angles\n",
    "    \"\"\"\n",
    "    images = np.empty([batch_size, IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNELS])\n",
    "    steers = np.empty(batch_size)\n",
    "    while True:\n",
    "        i = 0\n",
    "        for index in np.random.permutation(image_paths.shape[0]):\n",
    "            center, left, right = image_paths[index]\n",
    "            steering_angle = steering_angles[index]\n",
    "            # argumentation\n",
    "            if is_training and np.random.rand() < 0.6:\n",
    "                image, steering_angle = augument(data_dir, center, left, right, steering_angle)\n",
    "            else:\n",
    "                image = load_image(data_dir, center) \n",
    "            # add the image and steering angle to the batch\n",
    "            images[i] = preprocess(image)\n",
    "            steers[i] = steering_angle\n",
    "            i += 1\n",
    "            if i == batch_size:\n",
    "                break\n",
    "        yield images, steers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions - Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_dir, test_size):\n",
    "    \"\"\"\n",
    "    Load training data and split it into training and validation set\n",
    "    \"\"\"\n",
    "    data_df = pd.read_csv(os.path.join(data_dir, 'driving_log.csv'))\n",
    "\n",
    "    X = data_df[['center', 'left', 'right']].values\n",
    "    y = data_df['steering'].values\n",
    "\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=test_size, random_state=0)\n",
    "\n",
    "    return X_train, X_valid, y_train, y_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_shape):\n",
    "    \"\"\"\n",
    "    Comma.ai model\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(\n",
    "        Lambda(\n",
    "            lambda x: x/127.55 -1,\n",
    "            input_shape=input_shape\n",
    "        )\n",
    "    )\n",
    "    model.add(Conv2D(16, (8, 8), strides=(4, 4), padding=\"same\"))\n",
    "    model.add(ELU())\n",
    "    model.add(Conv2D(32, (5, 5), strides=(2, 2), padding=\"same\"))\n",
    "    model.add(ELU())\n",
    "    model.add(Conv2D(64, (5, 5), strides=(2, 2), padding=\"same\"))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(.2))\n",
    "    model.add(ELU())\n",
    "    model.add(Dense(512))\n",
    "    model.add(Dropout(.5))\n",
    "    model.add(ELU())\n",
    "    model.add(Dense(1))\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Training\n",
    "### Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To download and extract Sample Data\n",
    "file = download('https://d17h27t6h515a5.cloudfront.net/topher/2016/December/584f6edd_data/data.zip')\n",
    "\n",
    "# Extract the file\n",
    "with zipfile.ZipFile(file) as zf:\n",
    "    zf.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory for the training data\n",
    "data_dir = 'data'\n",
    "\n",
    "# Train/Test Split = 90/10\n",
    "test_size = 0.1\n",
    "\n",
    "# Load the data\n",
    "X_train, X_valid, y_train, y_valid = load_data(data_dir, test_size)\n",
    "\n",
    "# Image parameters\n",
    "IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNELS = 66, 200, 3\n",
    "IMAGE_SHAPE = (IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lambda_1 (Lambda)            (None, 66, 200, 3)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 17, 50, 16)        3088      \n",
      "_________________________________________________________________\n",
      "elu_1 (ELU)                  (None, 17, 50, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 9, 25, 32)         12832     \n",
      "_________________________________________________________________\n",
      "elu_2 (ELU)                  (None, 9, 25, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 5, 13, 64)         51264     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4160)              0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 4160)              0         \n",
      "_________________________________________________________________\n",
      "elu_3 (ELU)                  (None, 4160)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               2130432   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "elu_4 (ELU)                  (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 2,198,129\n",
      "Trainable params: 2,198,129\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build the model\n",
    "model = build_model(IMAGE_SHAPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model Checpoint\n",
    "checkpoint = ModelCheckpoint(\n",
    "    'model-{epoch:03d}.h5',\n",
    "    verbose=0,\n",
    "    save_best_only=1,\n",
    "    mode='auto'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model.compile(loss='mean_squared_error', optimizer=Adam(lr=1.0e-4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py:22: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py:22: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<generator..., 2000, 8, validation_data=<generator..., callbacks=[<keras.ca..., verbose=1, validation_steps=804, max_queue_size=1)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "2000/2000 [==============================] - 155s 78ms/step - loss: 0.0321 - val_loss: 0.0127\n",
      "Epoch 2/8\n",
      "2000/2000 [==============================] - 154s 77ms/step - loss: 0.0250 - val_loss: 0.0136\n",
      "Epoch 3/8\n",
      "2000/2000 [==============================] - 153s 77ms/step - loss: 0.0234 - val_loss: 0.0145\n",
      "Epoch 4/8\n",
      "2000/2000 [==============================] - 153s 77ms/step - loss: 0.0226 - val_loss: 0.0123\n",
      "Epoch 5/8\n",
      "2000/2000 [==============================] - 153s 77ms/step - loss: 0.0213 - val_loss: 0.0127\n",
      "Epoch 6/8\n",
      "2000/2000 [==============================] - 154s 77ms/step - loss: 0.0214 - val_loss: 0.0125\n",
      "Epoch 7/8\n",
      "2000/2000 [==============================] - 154s 77ms/step - loss: 0.0216 - val_loss: 0.0116\n",
      "Epoch 8/8\n",
      "2000/2000 [==============================] - 153s 77ms/step - loss: 0.0203 - val_loss: 0.0114\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe3469eee80>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model\n",
    "model.fit_generator(\n",
    "    batch_generator(\n",
    "        data_dir,\n",
    "        X_train,\n",
    "        y_train,\n",
    "        16,\n",
    "        True\n",
    "    ),\n",
    "    2000, # Samples per epoch\n",
    "    8, # Number of epochs\n",
    "    max_q_size=1,\n",
    "    validation_data=batch_generator(\n",
    "        data_dir,\n",
    "        X_valid,\n",
    "        y_valid,\n",
    "        16,\n",
    "        False\n",
    "    ),\n",
    "    nb_val_samples=len(X_valid),\n",
    "    callbacks=[checkpoint],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove any existing files from testing\n",
    "!rm /tmp/model.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate code that will go into SageMaker\n",
    "MODEL_NAME = 'model.h5'\n",
    "MODEL_PATH = '/tmp' # Will be '/opt/ml/model' in SageMaker\n",
    "\n",
    "# Simulate environmental variable in SageMAker Train\n",
    "model_dir = MODEL_PATH \n",
    "\n",
    "def save(model, model_dir):\n",
    "    print(\"Saving the trained model ...\")\n",
    "    model_path = os.path.join(model_dir)\n",
    "    files = list(glob.glob(os.path.join('./', '*.h5')))\n",
    "    \n",
    "    # Find the best model weights\n",
    "    best = max(files)\n",
    "    \n",
    "    # Rename the best weights\n",
    "    os.rename(best, MODEL_NAME)\n",
    "    \n",
    "    # Move to model_dir\n",
    "    shutil.move('./model.h5', model_dir+'/')\n",
    "    \n",
    "    # Save model graph to `.json`\n",
    "    model_json = model.to_json()\n",
    "    with open(model_dir+'/model.json', 'w') as outfile:\n",
    "        json.dump(model_json, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the trained model ...\n"
     ]
    }
   ],
   "source": [
    "save(model, model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 25980\r\n",
      "drwxrwxrwt  5 root       root         147456 Jul  3 20:31 .\r\n",
      "dr-xr-xr-x 26 root       root           4096 Jul  3 17:51 ..\r\n",
      "drwxr-xr-x  2 role-agent role-agent     4096 Jul  3 17:51 hsperfdata_role-agent\r\n",
      "drwxrwxrwt  2 root       root           4096 Jul  3 17:50 .ICE-unix\r\n",
      "drwxr-xr-x  3 role-agent role-agent     4096 Jul  3 17:51 jetty-0.0.0.0-9081-role-proxy-agent.war-_-any-9129967830017298108.dir\r\n",
      "-rw-rw-r--  1 ec2-user   ec2-user   26427216 Jul  3 20:31 model.h5\r\n",
      "-rw-rw-r--  1 ec2-user   ec2-user       4431 Jul  3 20:31 model.json\r\n"
     ]
    }
   ],
   "source": [
    "!ls -la /tmp/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
